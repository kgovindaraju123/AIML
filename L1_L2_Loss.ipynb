{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L1_L2_Loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgovindaraju123/AIML/blob/master/L1_L2_Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3myd-kMb3C4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from statsmodels.tools.eval_measures import rmse\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Make pylab inline and set the theme to 'ggplot'\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "# Read Boston Housing Data\n",
        "data = pd.read_csv('Housing.csv')\n",
        "\n",
        "# Create a data frame with all the independent features\n",
        "data_indep = data.drop('medv', axis = 1)\n",
        "\n",
        "# Create a target vector(vector of dependent variable, i.e. 'medv')\n",
        "data_dep = data['medv']\n",
        "\n",
        "# Split data into training and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(data_indep, data_dep,test_size = 0.20,random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN7-7bqTcxMH",
        "colab_type": "text"
      },
      "source": [
        "Regression without any Outliers:\n",
        "\n",
        "At this moment, our housing dataset is pretty much clean and doesn’t contain any outliers as such. So let’s fit a GB regressor with L1 and L2 loss functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVYphRqWcvnO",
        "colab_type": "code",
        "outputId": "0bfbf027-665a-46b4-c09e-5a109b699407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GradientBoostingRegressor with a L1(Least Absolute Deviations) loss function\n",
        "# Set a random seed so that we can reproduce the results\n",
        "np.random.seed(32767)\n",
        "\n",
        "mod = GradientBoostingRegressor(loss='lad')\n",
        "\n",
        "fit = mod.fit(train_X, train_y)\n",
        "predict = fit.predict(test_X)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "print(\"RMSE -> %f\" % rmse(predict, test_y))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE -> 3.372776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-UzbCVzdDzx",
        "colab_type": "text"
      },
      "source": [
        "With a L1 loss function and no outlier we get a value of RMSE: 3.440147. Let’s see what results we get with L2 loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SePIVoldElw",
        "colab_type": "code",
        "outputId": "fef3c155-4876-4c58-b4b4-131b7401025c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GradientBoostingRegressor with L2(Least Square errors) loss function\n",
        "mod = GradientBoostingRegressor(loss='ls')\n",
        "\n",
        "fit = mod.fit(train_X, train_y)\n",
        "predict = fit.predict(test_X)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "print (\"RMSE -> %f\" % rmse(predict, test_y))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE -> 2.502083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sce7okp_dMqo",
        "colab_type": "text"
      },
      "source": [
        "This prints out a mean squared value of RMSE -> 2.542019.\n",
        "\n",
        "As apparent from RMSE errors of L1 and L2 loss functions, Least Squares(L2) outperform L1, when there are no outliers in the data.\n",
        "Regression with Outliers:\n",
        "\n",
        "After looking at the minimum and maximum values of ‘medv’ column, we can see that the range of values in ‘medv’ is [5, 50].\n",
        "Let’s add a few Outliers in this Dataset, so that we can see some significant differences with L1 and L2 loss functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUVBsNEcdSTF",
        "colab_type": "code",
        "outputId": "5a9683a8-d200-4118-da17-45e61af8ea49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# Get upper and lower bounds[min, max] of all the features\n",
        "stats = data.describe()\n",
        "extremes = stats.loc[['min', 'max'],:].drop('medv', axis = 1)\n",
        "extremes"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>black</th>\n",
              "      <th>lstat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.385</td>\n",
              "      <td>3.561</td>\n",
              "      <td>2.9</td>\n",
              "      <td>1.1296</td>\n",
              "      <td>1.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>12.6</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>88.97620</td>\n",
              "      <td>100.0</td>\n",
              "      <td>27.74</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.871</td>\n",
              "      <td>8.780</td>\n",
              "      <td>100.0</td>\n",
              "      <td>12.1265</td>\n",
              "      <td>24.0</td>\n",
              "      <td>711.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>37.97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         crim     zn  indus  chas    nox  ...   rad    tax  ptratio   black  lstat\n",
              "min   0.00632    0.0   0.46   0.0  0.385  ...   1.0  187.0     12.6    0.32   1.73\n",
              "max  88.97620  100.0  27.74   1.0  0.871  ...  24.0  711.0     22.0  396.90  37.97\n",
              "\n",
              "[2 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhUec_DMdpaA",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to generate 5 random samples, such that their values lies in the [min, max] range of respective features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7NJMJ5gdlYC",
        "colab_type": "code",
        "outputId": "a647b930-b6b1-431c-91eb-b8de287efa33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Set a random seed\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Create 5 random values \n",
        "rands = np.random.rand(5, 1)\n",
        "rands\n",
        "\n",
        "# Get the 'min' and 'max' rows as numpy array\n",
        "min_array = np.array(extremes.loc[['min'], :])\n",
        "max_array = np.array(extremes.loc[['max'], :])\n",
        "\n",
        "# Find the difference(range) of 'max' and 'min'\n",
        "range = max_array - min_array\n",
        "\n",
        "# Generate 5 samples with 'rands' value\n",
        "outliers_X = (rands * range) + min_array\n",
        "outliers_X"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.70457825e+01, 1.91519450e+01, 5.68465061e+00, 1.91519450e-01,\n",
              "        4.78078453e-01, 4.56054001e+00, 2.14965386e+01, 3.23572024e+00,\n",
              "        5.40494736e+00, 2.87356192e+02, 1.44002828e+01, 7.62727836e+01,\n",
              "        8.67066488e+00],\n",
              "       [5.53552627e+01, 6.22108771e+01, 1.74311273e+01, 6.22108771e-01,\n",
              "        6.87344863e-01, 6.80778568e+00, 6.33067617e+01, 7.97086794e+00,\n",
              "        1.53085017e+01, 5.12984996e+02, 1.84478224e+01, 2.47035896e+02,\n",
              "        2.42752219e+01],\n",
              "       [3.89509044e+01, 4.37727739e+01, 1.24012127e+01, 4.37727739e-01,\n",
              "        5.97735681e-01, 5.84550107e+00, 4.54033635e+01, 5.94324817e+00,\n",
              "        1.10677380e+01, 4.16369335e+02, 1.67146407e+01, 1.73914067e+02,\n",
              "        1.75932533e+01],\n",
              "       [6.98795789e+01, 7.85358584e+01, 2.18845822e+01, 7.85358584e-01,\n",
              "        7.66684272e-01, 7.65978645e+00, 7.91583185e+01, 9.76610981e+00,\n",
              "        1.90632474e+01, 5.98527898e+02, 1.99823707e+01, 3.11777507e+02,\n",
              "        3.01913951e+01],\n",
              "       [6.94006741e+01, 7.79975808e+01, 2.17377400e+01, 7.79975808e-01,\n",
              "        7.64068243e-01, 7.63169374e+00, 7.86356510e+01, 9.70691596e+00,\n",
              "        1.89394436e+01, 5.95707323e+02, 1.99317726e+01, 3.09642806e+02,\n",
              "        2.99963233e+01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxIu0oSHdzhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will also create some hard coded outliers\n",
        "# for 'medv', i.e. our target\n",
        "medv_outliers = np.array([0, 0, 600, 700, 600])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oVAF_Xld4zT",
        "colab_type": "code",
        "outputId": "aded951f-6964-4476-8090-2350e9fe06c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "# Change the type of 'chas', 'rad' and 'tax' to rounded of Integers\n",
        "outliers_X[:, [3, 8, 9]] = np.int64(np.round(outliers_X[:, [3, 8, 9]]))\n",
        "\n",
        "# Finally concatenate our existing 'train_X' and\n",
        "# 'train_y' with these outliers\n",
        "train_X = np.append(train_X, outliers_X, axis = 0)\n",
        "train_y = np.append(train_y, medv_outliers, axis = 0)\n",
        "\n",
        "# Plot a histogram of 'medv' in train_y\n",
        "fig = plt.figure(figsize=(13,7))\n",
        "plt.hist(train_y, bins=50, range = (-10, 800))\n",
        "fig.suptitle('medv Count', fontsize = 20)\n",
        "plt.xlabel('medv', fontsize = 16)\n",
        "plt.ylabel('count', fontsize = 16)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAHhCAYAAAD+qFhoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZRedX0/8PczmYQQhkwyGbICYiAU\nA1jAhJ2EJVVRROqhqDlaFi3VFBBa7Q+tZalE02oMspVWMFSqVmxLUFziiUiiRjALiLJDg2CBLDMh\nJpiYZOb+/uBkypiA3zCTmQm8XufknDzfu33uZ/I8M+/c+51bq6qqCgAAQIG63i4AAADYeQgQAABA\nMQECAAAoJkAAAADFBAgAAKCYAAEAABQTIABeg/bZZ5/ss88+vV0GADshAQKAHeahhx7K+eefn4MO\nOiiNjY0ZMGBARo8enbe//e258cYb87vf/a63S/yDbrrpptRqtdx00029XQpAn1Df2wUA8Or0D//w\nD7n88svT3t6eo446KmeeeWYaGhqyfPny3HnnnfngBz+Yf/7nf87ixYt7u1QAtoMAAUC3+/SnP51L\nL700e+21V77xjW/kiCOO2Gqd22+/PTNnzuyF6gDoCrcwAewATzzxRGq1Ws4666w8/vjjOf300zNs\n2LDsvvvuefOb35xf/vKXSZKVK1fm3HPPzahRozJw4MBMnDgxP/zhD7e5z82bN+e6667LkUcemcGD\nB2fQoEE59NBDc80116S9vX2r9auqyjXXXJMDDzwwAwcOzJgxY3LeeedlzZo1W607Y8aM1Gq1fOEL\nX9jmsZ9++unU19dnwoQJRed+2WWXpX///vnOd76zzfCQJKecckq+973vbTV+yy23ZNKkSWlsbMyu\nu+6agw8+OJ/5zGe2ebtTrVbL8ccfv839n3XWWanVanniiSc61bbl6/LEE0/kPe95T5qbmzNw4MBM\nmDAht99+e6d9HH/88Tn77LOTJGeffXZqtVrHnxfvF+C1xBUIgB3oiSeeyBFHHJE3vOENHT+03nrr\nrTn++OPz05/+NG9961szePDgvPvd705ra2v+4z/+IyeffHIeeeSR7L333h372bRpU97xjndk7ty5\n+aM/+qNMnTo1AwcOzA9/+MOcf/75ufvuu3PzzTd3OvaFF16Yq666KqNGjcq5556b/v3757bbbsvd\nd9+djRs3ZsCAAR3rvv/978/f/d3f5ctf/nI+8pGPbHUe//7v/562tracddZZf/CcZ8+enU2bNuU9\n73lPDjrooJddd5dddun0+hOf+EQ+85nPpLm5OVOnTk1DQ0O++93v5hOf+ETmzp2b73//+53qfqV+\n9atf5fDDD8/YsWPz/ve/P62trfn617+ed77znZk3b15OOOGEJC+EkCFDhuS2227LO9/5zhxyyCEd\n+xgyZEiX6wDYKVUAdLtly5ZVSaok1RVXXNFp2T/8wz9USaqhQ4dWf/mXf1m1tbV1LPvyl79cJaku\nvPDCTttceumlVZLqvPPOqzZv3twxvnnz5uqcc86pklRz5szpGP/JT35SJan23XffqqWlpWN8/fr1\n1ZFHHlklqV73utd1Osab3/zmKkn1i1/8YqvzGT9+fDVgwIBq1apVf/DcTzzxxCpJ9cUvfvEPrvti\nCxcurJJUe+21V/XMM890jG/atKk65ZRTqiTV9OnTO22TpJo8efI293fmmWdWSaply5Z1jL3463LZ\nZZd1Wv973/telaQ6+eSTO43Pnj27SlLNnj17u84H4NXKLUwAO9A+++yTiy++uNPYmWeemST53e9+\nl89+9rOpq/u/j+KpU6emvr4+9957b8dYe3t7rr766owcOTKzZs1Kv379Opb169cvM2fOTK1Wy1e+\n8pWO8dmzZydJ/u7v/i5NTU0d4wMHDsxnPvOZbda6pa5/+7d/6zS+ePHiPPDAA3n729+eYcOG/cFz\nfuaZZ5Ike+655x9c98W+9KUvJUk++clPZuTIkR3j9fX1mTlzZurq6nLDDTds1z5fyute97p88pOf\n7DT2lre8JXvvvXd+9rOfdcsxAF6t3MIEsAMdcsghnX7gT5LRo0cnSfbff//svvvunZb169cvI0aM\nyK9//euOsUceeSStra0ZN25crrjiim0eZ9ddd82DDz7Y8Xrp0qVJksmTJ2+17rHHHrtVTUnyp3/6\np2lsbMxXvvKVzJgxo2OdLYGi5PalrthS84knnrjVsv333z977rlnli1bljVr1qSxsbFLx9rW1yVJ\n9tprr/z0pz/t0r4BXu0ECIAdaFs/6NbX17/ksi3LN23a1PG6paUlSfLoo4/m8ssvf8ljrVu3ruPv\nWyZKjxgxYpv7b25u3mp81113zRlnnJEvfvGL+f73v5+TTz45GzduzNe+9rXsscceOfnkk1/y2C82\natSoPPjgg/nf//3fovV/v+ZRo0a95H6ffPLJPPfcc10OEC81f6G+vn6bE9IB+D9uYQLo47b8sPyn\nf/qnqarqJf8sW7Zsq22WL1++1f42b96cVatWbfNYv38b07e//e20tLRk6tSp6d+/f1G9xx57bJLk\nBz/4QeEZdq752Wef3ebyLbdGvTg81Gq1bN68eZvrP/fcc9t1fADKCBAAfdwBBxyQIUOG5K677up0\nZeLlHHbYYUmS+fPnb7Xsxz/+cdra2ra53THHHJNx48bltttuy5o1azqCxJZgUeLss89O//7981//\n9V954IEHXnbdF/9q1kMPPTRJcuedd2613mOPPZZf//rXef3rX9/p6sHQoUPz1FNPbbV+W1tbp3kk\nXbHlVqeX6hnAa40AAdDH1dfX5/zzz88zzzyTCy64IOvXr99qnWeeeabTD+tb5itMnz49ra2tHeMb\nNmzIxz/+8Zc93plnnpkNGzbkuuuuy3e+85288Y1v7PjhvsQ+++yTyy67LBs3bszb3/72l3zS9Pe+\n971Ot0Wdc845SZIrrrgiK1eu7Bhva2vLRz/60bS3t+cDH/hAp30cfvjhefLJJ/P973+/0/gVV1yR\nX/3qV8U1v5wtE8effPLJbtkfwM7OHAiAncDf//3f5+c//3muv/76fOtb38qJJ56YMWPGZMWKFXn0\n0Ufzk5/8JNOnT8/48eOTvHAl4fzzz8/VV1+dgw46KKeffnrHcyCGDh36kvMMkheeCXHJJZfk0ksv\nzaZNm7br6sMWn/jEJ7J58+ZcfvnlmThxYo4++uhMmDAhDQ0NWb58eRYsWJBHH32004Ppjj766Pzt\n3/5t/umf/qmj5t122y3f/e5388tf/jLHHntsPvaxj3U6zkc/+tHMnTs373znO/Pud787TU1NWbhw\nYZYtW5bjjz9+m1czttdRRx2VQYMG5corr0xLS0vHb4g6//zzuzwXA2Cn1Iu/QhbgVWvL8wbOPPPM\nbS7Pyzy/4HWve91Wz2ioqqpqb2+vvvzlL1cnnnhiNXTo0Kp///7V6NGjq2OOOaaaPn169eSTT261\n/tVXX10dcMAB1YABA6pRo0ZV06ZNq5577rmXPMYWJ510UpWkqq+vr5599tnCs97aAw88UJ133nnV\ngQceWO2+++5V//79q5EjR1ZvfetbqxtuuKHasGHDVtt87Wtfq4455piqoaGh2mWXXarx48dXV1xx\nRbV+/fptHuO2226r3vSmN1W77LJL1dTUVL373e+unnjiiZd9DsRLfV0mT55cbetb43e/+93qyCOP\nrHbbbbeO50i8eL8AryW1qqqq3osvAADAzsQcCAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAA\nAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAA\nAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAA\nFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYvW9XcCO\n8PTTT/d2CS+rubk5q1at6u0ydnr62HV62D30sev0sOv0sHvoY9fpYffoC30cPXr0NsddgQAAAIoJ\nEAAAQDEBAgAAKCZAAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIoJEAAAQDEBAgAAKCZA\nAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFCsvrcL4AVtf3HqK9qu3xe/2c2VAADAS3MFAgAAKCZA\nAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIoJEAAAQDEBAgAAKCZAAAAAxQQIAACgmAAB\nAAAUEyAAAIBiAgQAAFBMgAAAAIoJEAAAQDEBAgAAKCZAAAAAxQQIAACgmAABAAAUEyAAAIBiAgQA\nAFBMgAAAAIoJEAAAQDEBAgAAKCZAAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIoJEAAA\nQDEBAgAAKCZAAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIrV9+TBVq1alWuvvTbPPfdc\narVapkyZkre97W1Zt25dZs2alZUrV2aPPfbIRRddlIaGhlRVldmzZ+eee+7JLrvskmnTpmXs2LE9\nWTIAAPAiPXoFol+/fnn/+9+fWbNmZfr06Zk7d25+/etfZ86cOTn44INz1VVX5eCDD86cOXOSJPfc\nc0+effbZXHXVVTn33HNzww039GS5AADA7+nRADF06NCOKwi77rprxowZk9bW1ixatCiTJ09Okkye\nPDmLFi1KkixevDiTJk1KrVbL/vvvn+effz6rV6/uyZIBAIAX6bU5ECtWrMiyZcuy3377Zc2aNRk6\ndGiSZMiQIVmzZk2SpLW1Nc3NzR3bDBs2LK2trb1SLwAA0MNzILbYsGFDZs6cmbPOOiuDBg3qtKxW\nq6VWq23X/ubNm5d58+YlSWbMmNEpdPRF9fX1W9W4/BXuq6+f6460rT6yffSwe+hj1+lh1+lh99DH\nrtPD7tGX+9jjAWLz5s2ZOXNmjjvuuBxxxBFJksbGxqxevTpDhw7N6tWrM3jw4CRJU1NTVq1a1bFt\nS0tLmpqattrnlClTMmXKlI7XL96mL2pubu62Gvv6ue5I3dnH1yo97B762HV62HV62D30sev0sHv0\nhT6OHj16m+M9egtTVVW5/vrrM2bMmJxyyikd4xMmTMj8+fOTJPPnz8/EiRM7xhcsWJCqqvLII49k\n0KBBHbc6AQAAPa9Hr0A8/PDDWbBgQfbee+987GMfS5K8973vzWmnnZZZs2bljjvu6Pg1rkly6KGH\nZunSpbngggsyYMCATJs2rSfLBQAAfk+PBogDDjggt9xyyzaXXXLJJVuN1Wq1fPCDH9zRZQEAAIU8\niRoAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUE\nCAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMg\nAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAA\nAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIA\nACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAA\noJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACA\nYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACK\n1ffkwa677rosXbo0jY2NmTlzZpLklltuyQ9+8IMMHjw4SfLe9743hx12WJLk1ltvzR133JG6urqc\nffbZOeSQQ3qyXAAA4Pf0aIA4/vjj89a3vjXXXnttp/G3v/3tOfXUUzuN/frXv87ChQvz+c9/PqtX\nr86nPvWpfOELX0hdnYsmAADQW3r0p/Hx48enoaGhaN1Fixbl6KOPTv/+/TN8+PCMHDkyjz322A6u\nEAAAeDk9egXipcydOzcLFizI2LFj8+d//udpaGhIa2trxo0b17FOU1NTWltbt7n9vHnzMm/evCTJ\njBkz0tzc3CN1v1L19fVb1bj8Fe6rr5/rjrStPrJ99LB76GPX6WHX6WH30Meu08Pu0Zf72OsB4s1v\nfnNOP/30JMnXv/71fPnLX860adO2ax9TpkzJlClTOl6vWrWqW2vsbs3Nzd1WY18/1x2pO/v4WqWH\n3UMfu04Pu04Pu4c+dp0edo++0MfRo0dvc7zXJxQMGTIkdXV1qaury0knnZTHH388yQtXHFpaWjrW\na21tTVNTU2+VCQAApA8EiNWrV3f8/Wc/+1n22muvJMmECROycOHCbNq0KStWrMgzzzyT/fbbr7fK\nBAAA0sO3MF155ZV54IEHsnbt2nzoQx/KGWeckfvvvz9PPPFEarVa9thjj5x77rlJkr322itHHXVU\n/vqv/zp1dXX5wAc+4DcwAQBAL+vRAHHhhRduNXbiiSe+5Prvete78q53vWtHlrTTa/uLU//wSr+n\n3xe/uQMqAQDgtcB/6QMAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgm\nQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgA\nAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAECx4gBx3nnn5YknntjmsieffDLnnXded9UEAAD0UcUB\nYuXKldm8efM2l23atCkrV67stqIAAIC+qVtuYXr88cez2267dceuAACAPqz+5Rbefvvt+fa3v93x\n+h//8R9TX995k40bN2bdunU55phjdkyFAABAn/GyAWLEiBE5+OCDkyTz58/Pvvvum8GDB3feQX19\n9txzz5x00kk7rkoAAKBPeNkAMXHixEycOLHj9emnn57hw4fv8KIAAIC+6WUDxItNmzZtR9YBAADs\nBIoDRJIsX748P/3pT7Nq1aps3Lix07JarZYPf/jD3VocAADQtxQHiJ/97GeZNWtWqqpKY2PjVpOp\na7VatxcHAAD0LcUB4utf/3oOPPDAXHDBBVtNpAYAAF4bip8DsWLFirzjHe8QHgAA4DWsOECMHj06\na9eu3ZG1AAAAfVxxgHjf+96XW2+9NcuXL9+R9QAAAH1Y8RyIb3zjG1m3bl0uuuiijBo1Kg0NDVut\nc/nll3drcQAAQN9SHCDq6uoyevToHVkLAADQxxUHiMsuu2wHlgEAAOwMiudAAAAAFF+BeOCBB/7g\nOuPHj+9SMQAAQN9WHCBKJkh//etf71IxAABA31YcIC699NKtxtauXZslS5bkwQcfzDnnnNOthQEA\nAH1PcYB4qduTjjjiiNx0001ZsmRJDj300G4rDAAA6Hu6ZRL1YYcdlp/+9KfdsSsAAKAP65YA8fTT\nT6dWq3XHrgAAgD6s+Bam+fPnbzW2efPmPPXUU7njjjty+OGHd2thAABA31McIK677rpt76C+Pkcf\nfXTOPvvsbisKAADom4oDxDXXXLPVWP/+/TNkyJBuLQgAAOi7igPEHnvssSPrAAAAdgLFAWKLJUuW\n5IEHHsi6devS0NCQAw88MIcddtiOqA0AAOhjigPE+vXrM2PGjDz00EOpq6vL7rvvnrVr1+b222/P\nG97whlx88cUZOHDgjqwVAADoZcUB4mtf+1qWLVuW8847L8ccc0zq6urS3t6en/zkJ7nhhhvy1a9+\n1dOoAQDgVa74ORB333133vOe9+S4445LXd0Lm9XV1eW4447Lu9/97tx99907rEgAAKBvKA4Qa9eu\nzZ577rnNZXvuuWfWrVvXbUUBAAB9U3GAGD58eJYsWbLNZUuXLs3w4cO7rSgAAKBvKp4DMWXKlNx8\n883ZsGFDjjvuuAwZMiTPPfdcFi5cmB/84Ac588wzd2SdAABAH1AcIE455ZT85je/ybe//e3ceeed\n/7eD+vqcdtppedvb3rYj6gMAAPqQ7XoOxNSpU3Pqqafm0Ucf7XgOxLhx49LQ0LCj6gMAAPqQ4gAx\nZ86ctLa25pxzzsmhhx7aadmXvvSlNDc359RTT+32AgEAgL6jeBL1nXfemb333nuby/bZZ5/88Ic/\n7LaiAACAvqk4QKxatSqjRo3a5rIRI0Zk1apV3VYUAADQNxUHiF122SWtra3bXNbS0pL6+u2aTgEA\nAOyEigPEAQcckG9+85vZtGlTp/FNmzbl9ttvzxve8IZuLw4AAOhbii8b/Nmf/Vn+/u//Ph/5yEdy\n3HHHpampKa2trfnRj36UtWvXZtq0aTuyTgAAoA8oDhD77LNPLr300tx888257bbbUlVVarVaDjjg\ngPzN3/xN9tlnnx1YJgAA0Bds18SF/fbbL5dffnk2btzY8RyIAQMG7KjaAACAPuYVzXweMGBAmpqa\nursWAACgjyueRA0AACBAAAAAxXr04Q3XXXddli5dmsbGxsycOTNJsm7dusyaNSsrV67MHnvskYsu\nuigNDQ2pqiqzZ8/OPffck1122SXTpk3L2LFje7JcAADg9/ToFYjjjz8+n/jEJzqNzZkzJwcffHCu\nuuqqHHzwwZkzZ06S5J577smzzz6bq666Kueee25uuOGGniwVAADYhh4NEOPHj09DQ0OnsUWLFmXy\n5MlJksmTJ2fRokVJksWLF2fSpEmp1WrZf//98/zzz2f16tU9WS4AAPB7en0OxJo1azJ06NAkyZAh\nQ7JmzZokSWtra5qbmzvWGzZsWFpbW3ulRgAA4AU9OgfiD6nVaqnVatu93bx58zJv3rwkyYwZMzoF\nj76ovr5+qxqX9+Dx+3p/Sm2rj2wfPewe+th1eth1etg99LHr9LB79OU+9nqAaGxszOrVqzN06NCs\nXr06gwcPTpI0NTVl1apVHeu1tLS85LMnpkyZkilTpnS8fvF2fVFzc3Ov1tjX+1Oqt/v4aqCH3UMf\nu04Pu04Pu4c+dp0edo++0MfRo0dvc7zXb2GaMGFC5s+fnySZP39+Jk6c2DG+YMGCVFWVRx55JIMG\nDeq41QkAAOgdPXoF4sorr8wDDzyQtWvX5kMf+lDOOOOMnHbaaZk1a1buuOOOjl/jmiSHHnpoli5d\nmgsuuCADBgzItGnTerJUAABgG3o0QFx44YXbHL/kkku2GqvVavngBz+4o0sCAAC2Q6/fwgQAAOw8\nBAgAAKCYAAEAABQTIAAAgGICBAAAUEyAAAAAigkQAABAMQECAAAoJkAAAADFBAgAAKCYAAEAABQT\nIAAAgGICBAAAUEyAAAAAigkQAABAMQECAAAoJkAAAADFBAgAAKCYAAEAABQTIAAAgGICBAAAUEyA\nAAAAigkQAABAMQECAAAoJkAAAADFBAgAAKCYAAEAABQTIAAAgGICBAAAUEyAAAAAigkQAABAMQEC\nAAAoJkAAAADFBAgAAKCYAAEAABQTIAAAgGICBAAAUEyAAAAAigkQAABAMQECAAAoJkAAAADFBAgA\nAKCYAAEAABQTIAAAgGICBAAAUEyAAAAAigkQAABAMQECAAAoJkAAAADFBAgAAKCYAAEAABQTIAAA\ngGICBAAAUEyAAAAAigkQAABAMQECAAAoJkAAAADFBAgAAKCYAAEAABQTIAAAgGICBAAAUEyAAAAA\nigkQAABAMQECAAAoJkAAAADFBAgAAKCYAAEAABQTIAAAgGICBAAAUEyAAAAAigkQAABAsfreLmCL\nv/qrv8rAgQNTV1eXfv36ZcaMGVm3bl1mzZqVlStXZo899shFF12UhoaG3i4VAABes/pMgEiSSy+9\nNIMHD+54PWfOnBx88ME57bTTMmfOnMyZMyfve9/7erFCAAB4bevTtzAtWrQokydPTpJMnjw5ixYt\n6uWKAADgta1PXYGYPn16kuRP/uRPMmXKlKxZsyZDhw5NkgwZMiRr1qzpzfIAAOA1r88EiE996lNp\namrKmjVrcsUVV2T06NGdltdqtdRqtW1uO2/evMybNy9JMmPGjDQ3N+/weruivr5+qxqX9+Dx+3p/\nSm2rj2wfPewe+th1eth1etg99LHr9LB79OU+9pkA0dTUlCRpbGzMxIkT89hjj6WxsTGrV6/O0KFD\ns3r16k7zI15sypQpmTJlSsfrVatW9UjNr1Rzc3Ov1tjX+1Oqt/v4aqCH3UMfu04Pu04Pu4c+dp0e\ndo++0Mff/w/9LfrEHIgNGzZk/fr1HX+/7777svfee2fChAmZP39+kmT+/PmZOHFib5YJAACveX3i\nCsSaNWvyuc99LknS1taWY489Noccckj23XffzJo1K3fccUfHr3EFAAB6T58IECNGjMhnP/vZrcZ3\n3333XHLJJb1QEQAAsC194hYmAABg5yBAAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIoJ\nEAAAQDEBAgAAKCZAAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIoJEAAAQDEBAgAAKCZA\nAAAAxQQIAACgmAABAAAUEyAAAIBiAgQAAFBMgAAAAIoJEAAAQDEBAgAAKCZAAAAAxQQIAACgmAAB\nAAAUEyAAAIBiAgQAAFCsvrcLoOe1/cWp271Nvy9+cwdUAgDAzsYVCAAAoJgAAQAAFBMgAACAYgIE\nAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAA\nAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAA\nAMUECAAAoJgAAQAAFBMgAACAYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAA\nFBMgAACAYgIEAABQTIAAAACK1fd2Aa9GbX9x6ssuX95DdQAAQHdzBQIAACgmQAAAAMV2iluY7r33\n3syePTvt7e056aSTctppp5B88wMAAA3gSURBVPV2SQAA8JrU5wNEe3t7brzxxnzyk5/MsGHD8vGP\nfzwTJkzInnvu2dulAQA7kT80RzHZep5ivy9+c8cUAzuxPh8gHnvssYwcOTIjRoxIkhx99NFZtGiR\nAAEA0ENKwtcWW0KY8PXq1efnQLS2tmbYsGEdr4cNG5bW1tZerAgAAF67+vwViBLz5s3LvHnzkiQz\nZszI6NGje7egby/u3eO/hvT61/pVQA+7hz52nR52nR7+Ab4/v3J61yv66nu6z1+BaGpqSktLS8fr\nlpaWNDU1dVpnypQpmTFjRmbMmNHT5b0iF198cW+X8Kqgj12nh91DH7tOD7tOD7uHPnadHnaPvtzH\nPh8g9t133zzzzDNZsWJFNm/enIULF2bChAm9XRYAALwm9flbmPr165dzzjkn06dPT3t7e0444YTs\ntddevV0WAAC8JvW77LLLLuvtIv6QUaNG5eSTT87b3va2vOENb+jtcrrF2LFje7uEVwV97Do97B76\n2HV62HV62D30sev0sHv01T7WqqqqersIAABg59Dn50AAAAB9R5+fA/Fqc++992b27Nlpb2/PSSed\nlNNOO623S+qTrrvuuixdujSNjY2ZOXNmkmTdunWZNWtWVq5cmT322CMXXXRRGhoaUlVVZs+enXvu\nuSe77LJLpk2b1mcv+fWkVatW5dprr81zzz2XWq2WKVOm5G1ve5s+bqeNGzfm0ksvzebNm9PW1pYj\njzwyZ5xxRlasWJErr7wya9euzdixY3P++eenvr4+mzZtyjXXXJP/+Z//ye67754LL7www4cP7+3T\n6BPa29tz8cUXp6mpKRdffLEevgJ/9Vd/lYEDB6auri79+vXLjBkzvKe30/PPP5/rr78+Tz31VGq1\nWj784Q9n9OjRergdnn766cyaNavj9YoVK3LGGWdk8uTJ+rgdbr/99txxxx2p1WrZa6+9Mm3atDz3\n3HM7x+diRY9pa2urzjvvvOrZZ5+tNm3aVH30ox+tnnrqqd4uq0+6//77q8cff7z667/+646xm2++\nubr11lurqqqqW2+9tbr55purqqqqJUuWVNOnT6/a29urhx9+uPr4xz/eKzX3Na2trdXjjz9eVVVV\n/fa3v60uuOCC6qmnntLH7dTe3l6tX7++qqqq2rRpU/Xxj3+8evjhh6uZM2dWP/7xj6uqqqp/+Zd/\nqebOnVtVVVV973vfq/7lX/6lqqqq+vGPf1x9/vOf753C+6Bvfetb1ZVXXll95jOfqaqq0sNXYNq0\nadWaNWs6jXlPb5+rr766mjdvXlVVL7yn161bp4dd0NbWVn3wgx+sVqxYoY/boaWlpZo2bVr1u9/9\nrqqqFz4Pf/jDH+40n4tuYepBjz32WEaOHJkRI0akvr4+Rx99dBYtWtTbZfVJ48ePT0NDQ6exRYsW\nZfLkyUmSyZMnd/Ru8eLFmTRpUmq1Wvbff/88//zzWb16dY/X3NcMHTq04394dt1114wZMyatra36\nuJ1qtVoGDhyYJGlra0tbW1tqtVruv//+HHnkkUmS448/vlMfjz/++CTJkUcemV/+8pepTDVLS0tL\nli5dmpNOOilJUlWVHnYT7+lyv/3tb/Pggw/mxBNPTJLU19dnt91208Mu+MUvfpGRI0dmjz320Mft\n1N7eno0bN6atrS0bN27MkCFDdprPRbcw9aDW1tYMGzas4/WwYcPy6KOP9mJFO5c1a9Zk6NChSZIh\nQ4ZkzZo1SV7oa3Nzc8d6w4YNS2tra8e6vHB5edmyZdlvv/308RVob2/P//t//y/PPvts3vKWt2TE\niBEZNGhQ+vXrl+SFB162trYm6fw+79evXwYNGpS1a9dm8ODBvVZ/X3DTTTflfe97X9avX58kWbt2\nrR6+QtOnT0+S/Mmf/EmmTJniPb0dVqxYkcGDB+e6667Lr371q4wdOzZnnXWWHnbBT37ykxxzzDFJ\nfJ/eHk1NTXnHO96RD3/4wxkwYED++I//OGPHjt1pPhcFCHZKtVottVqtt8vYKWzYsCEzZ87MWWed\nlUGDBnVapo9l6urq8tnPfjbPP/98Pve5z+Xpp5/u7ZJ2KkuWLEljY2PGjh2b+++/v7fL2al96lOf\nSlNTU9asWZMrrrgio0eP7rTce/rltbW1ZdmyZTnnnHMybty4zJ49O3PmzOm0jh6W27x5c5YsWZKp\nU6dutUwfX966deuyaNGiXHvttRk0aFA+//nP59577+3tsooJED2oqakpLS0tHa9bWlrS1NTUixXt\nXBobG7N69eoMHTo0q1ev7kjdTU1NWbVqVcd6+vp/Nm/enJkzZ+a4447LEUcckUQfu2K33XbLgQce\nmEceeSS//e1v09bWln79+qW1tbWjV1ve58OGDUtbW1t++9vfZvfdd+/lynvXww8/nMWLF+eee+7J\nxo0bs379+tx00016+Aps6VFjY2MmTpyYxx57zHt6OwwbNizDhg3LuHHjkrxwK8icOXP08BW65557\n8vrXvz5DhgxJ4vvL9vjFL36R4cOHd/ToiCOOyMMPP7zTfC6aA9GD9t133zzzzDNZsWJFNm/enIUL\nF2bChAm9XdZOY8KECZk/f36SZP78+Zk4cWLH+IIFC1JVVR555JEMGjToNX1ZdIuqqnL99ddnzJgx\nOeWUUzrG9XH7/OY3v8nzzz+f5IXfyHTfffdlzJgxOfDAA3PXXXclSe68886O9/Kb3vSm3HnnnUmS\nu+66KwceeOBr/n/hpk6dmuuvvz7XXnttLrzwwhx00EG54IIL9HA7bdiwoeMWsA0bNuS+++7L3nvv\n7T29HYYMGZJhw4Z1XEX8xS9+kT333FMPX6EX376U+P6yPZqbm/Poo4/md7/7Xaqq6vi3uLN8LnqQ\nXA9bunRp/u3f/i3t7e054YQT8q53vau3S+qTrrzyyjzwwANZu3ZtGhsbc8YZZ2TixImZNWtWVq1a\ntdWvh7vxxhvz85//PAMGDMi0adOy77779vYp9LqHHnool1xySfbee++OD5n3vve9GTdunD5uh1/9\n6le59tpr097enqqqctRRR+X000/P8uXLc+WVV2bdunV5/etfn/PPPz/9+/fPxo0bc80112TZsmVp\naGjIhRdemBEjRvT2afQZ999/f771rW/l4osv1sPttHz58nzuc59L8sKtOMcee2ze9a53Ze3atd7T\n2+GJJ57I9ddfn82bN2f48OGZNm1aqqrSw+20YcOGTJs2Lddcc03H7bH+LW6fW265JQsXLky/fv2y\nzz775EMf+lBaW1t3is9FAQIAACjmFiYAAKCYAAEAABQTIAAAgGICBAAAUEyAAAAAigkQAPRJ999/\nf8444wxPrwboYwQIAACgmAABAAAUq+/tAgDo22655Zb853/+Z2bNmpWbbropDz30UBoaGnLGGWfk\nhBNOyIIFC/Lf//3faWlpyX777Ze//Mu/zMiRIzu2nzdvXubOnZunn346AwcOzIQJE/L+978/DQ0N\nHev85je/yezZs7N06dLUarVMmDAhhx9+eKc6brjhhtx99925/vrr069fv47xTZs25dxzz82kSZNy\n9tln7/iGALzGuQIBQJHPf/7zOfTQQ/Oxj30sY8eOzT//8z/nq1/9ar7//e9n6tSpmTZtWp5++ulc\nddVVHdt85StfyY033piDDz44f/u3f5v3ve99uffee/PpT3867e3tHet97nOfy9KlS/Pe9743F154\nYerq6jJ79uxOx580aVLWrFmTn//8553GlyxZkueffz6TJ0/esQ0AIIkrEAAUOvXUUzt+SB87dmyW\nLFmSefPm5ZprrsmgQYOSJKtXr85NN92UlStXpqqqfPOb38yf/dmf5fTTT+/Yz6hRo3LJJZdk8eLF\nOfzww3PffffloYceykc+8pEcc8wxSZJDDjkkn/70p9PS0tKx3f77759Ro0ZlwYIFOeywwzrGFyxY\nkDFjxmTs2LE90QaA1zxXIAAocuihh3b8vaGhIY2NjRk3blxHeEiSMWPGJElaWlpy3333paqqHHvs\nsWlra+v4M27cuOy666558MEHkySPPPJI6urqcsQRR3Q63tFHH71VDccdd1wWL16c9evXJ0nWrl2b\ne+65J5MmTer28wVg21yBAKDIbrvt1ul1fX39NseSZOPGjfnNb36TJLngggu2ub+1a9cmeeGqxW67\n7dax7RZDhgzZaptJkyblG9/4Ru66666ccMIJWbhwYdrb23Pccce9spMCYLsJEADsEFsmSX/yk5/c\nKmgkye67754kGTp0aJ5//vls3ry5U4h47rnnttpm+PDh+aM/+qP86Ec/ygknnJAf/ehHGT9+fJqb\nm3fQWQDw+9zCBMAO8cY3vjG1Wi2rVq3Kvvvuu9Wf4cOHJ3lhbkN7e3vuvvvuTtsvXLhwm/udNGlS\n7r///tx///155JFH3L4E0MNcgQBghxg5cmTe+c535sYbb8zTTz+d8ePHp3///h3zI0488cQcdNBB\neeMb35gDDjgg//qv/5q1a9dm5MiRWbhwYZ566qlt7veoo47K7Nmzc/XVV2fAgAE58sgje/jMAF7b\nBAgAdpipU6dmzz33zNy5czN37twkSXNzcw466KCMGjWqY72PfvSj+dKXvpSvfvWrqaury4QJE3LO\nOefks5/97Fb73G233fKmN70pd911V4455pjsuuuuPXY+ACS1qqqq3i4CAADYOZgDAQAAFBMgAACA\nYgIEAABQTIAAAACKCRAAAEAxAQIAACgmQAAAAMUECAAAoJgAAQAAFPv/eNe/gxDMUCIAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 936x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j03UxsMNeK1w",
        "colab_type": "text"
      },
      "source": [
        "You can see there are some clear outliers at 600, 700 and even one or two ‘medv’ values are 0.\n",
        "Since, our outliers are in place now, we will once again fit the GradientBoostingRegressor with L1 and L2 Loss functions to see the contrast in their performances with outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWIol8JPeL2m",
        "colab_type": "code",
        "outputId": "7db0bacf-9a7c-4a59-e72d-091d707a5a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GradientBoostingRegressor with L1 loss function\n",
        "np.random.seed(9876)\n",
        "\n",
        "mod = GradientBoostingRegressor(loss='lad')\n",
        "\n",
        "fit = mod.fit(train_X, train_y)\n",
        "predict = fit.predict(test_X)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "print (\"RMSE -> %f\" % rmse(predict, test_y))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE -> 3.659125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1q6BRueTIQ",
        "colab_type": "code",
        "outputId": "bf325f45-1964-44aa-97d2-27f0fdf4a61b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GradientBoostingRegressor with L2 loss function\n",
        "mod = GradientBoostingRegressor(loss='ls')\n",
        "\n",
        "fit = mod.fit(train_X, train_y)\n",
        "predict = fit.predict(test_X)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "print (\"RMSE -> %f\" % rmse(predict, test_y))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE -> 9.779870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJnjGuYlesr0",
        "colab_type": "text"
      },
      "source": [
        "On the other hand, we get a RMSE value of 9.806251, with L2 loss function and existing outliers.\n",
        "\n",
        "With outliers in the dataset, a L2(Loss function) tries to adjust the model according to these outliers on the expense of other good-samples, since the squared-error is going to be huge for these outliers(for error > 1). On the other hand L1(Least absolute deviation) is quite resistant to outliers.\n",
        "As a result, L2 loss function may result in huge deviations in some of the samples which results in reduced accuracy.\n",
        "\n",
        "So, if you can ignore the ouliers in your dataset or you need them to be there, then you should be using a L1 loss function, on the other hand if you don’t want undesired outliers in the dataset and would like to use a stable solution then first of all you should try to remove the outliers and then use a L2 loss function. Or performance of a model with a L2 loss function may deteriorate badly due to the presence of outliers in the dataset."
      ]
    }
  ]
}